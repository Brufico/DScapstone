---
title: "Text Mining Infrastructure in R"
author: "Ingo Feinerer , Kurt Hornik, David Meyer: Wirtschaftsuniversitaet Wien, notes by Bruno Fischer Colonimos"
date: "26 octobre 2017"
output: 
    pdf_document:
        number_sections: yes
        toc: yes
        toc_depth: 3
    html_document:
        number_sections: yes
        toc: yes
        toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

```{r textfunctions, echo=FALSE, eval=TRUE}

# context-detection functions
# ============================

# getcontext returns the doc conversion context
# From Hadley's ggplot2 book:  Knowing conversion target 
is_latex <- function() {
        identical(knitr::opts_knit$get("rmarkdown.pandoc.to"), "latex")
}

# html same
is_html <- function() {
        identical(knitr::opts_knit$get("rmarkdown.pandoc.to"), "html")
}

getcontext <- function(){
        if (is_latex()) {"latex" 
                   } else if (is_html()) {
                           "html"
                   } else {"other"} 
}

# tries
# message(getcontext())



# text formatting functions
# ===========================

# list of data and functions for text formatting
xtag <- list(explanation = "Tagging data and functionslist")


# function factory

# example tagdef
# exta <-  list(context1 = list(prefix = "theprefix_", suffix =  "_thesuffix"),
#               context1 = list(prefix = "theprefix2_", suffix =  "_thesuffix2")
#               )
# names(exta)
# exta[["context1"]]$prefix


xtag$maketagfun <- function(tagdef) {
        function( ..., context = getcontext()){
                if(context %in% names(tagdef)) {
                        prefix <- tagdef[[context]]$prefix
                        suffix <- tagdef[[context]]$suffix
                } else {
                        prefix <- ""
                        suffix <- ""
                }
                paste0(prefix, ..., suffix)
        }
}

# one definition
xtag$center <- xtag$maketagfun(tagdef = list(
        html = list(prefix = "<center>", suffix = "</center>"),
        latex = list(prefix = "{\\centering", suffix = "}") ))


# testing

# xtag$center
# 
# xtag$center("oooo",  context = "latex")
# xtag$center( "oooo", context = "html")
# xtag$center( "oooo", context = "word")
# xtag$center("oooo", "_aaaaa")

```


------------------------------------------------------------

Introduction
============================================================

Text mining = text as input information

The benefit of text mining comes with the large amount of valuable information latent in texts which is not available in classical structured data formats.


Classical applications in text mining 
-------------------------------------

* document clustering 
    * Typical applications : grouping news articles or information service documents
* document classification
    * e-mail filters , automatic labeling of documents in business libraries

For both, the idea is to transform the text into a structured format based on term frequencies and subsequently apply standard data mining techniques.

Specific distance measures like the Cosine, play an important role

More innovative text mining methods:
------------------------------------
* e.g., in linguistic stylometry (probability that a specic author wrote a specific text) 
* in search engines for learning rankings of documents from search engine logs of user behavior

* Latest developments in document exchange => valuable concepts for automatic handling of texts. The 'semantic web' propagates standardized formats for document exchange to enable agents to perform semantic operations on them. This is implemented by providing metadata and by annotating the text with tags. 
* One key format is RDF This development offers great flexibility in document exchange. But with the growing
popularity of XML based formats (e.g., RDF/XML as a common representation for RDF) tools need to be able to handle XML documents and metadata.


Statistical contexts
--------------------
Statistical contexts for text mining applications in research and business intelligence:

* latent semantic analysis techniques in bioinformatics
* the usage of statistical methods for automatically investigating jurisdictions
* plagiarism detection in universities and publishing houses, 
* computer assisted cross-language information retrieval
* adaptive spam filters learning via statistical inference.
* help desk inquiries (Sakurai and Suyama 2005)
* measuring customer preferences by analyzing qualitative interviews
* automatic grading (Wu and Chen 2005)
* fraud detection by investigating notfication of claims
* parsing social network sites for specific patterns such as ideas for new products

Almost every major statistical computing product offers text mining capabilities,

### capabilites and features:

* Preprocess: data preparation, importing, cleaning and general preprocessing,
* Associate: association analysis, that is fnding associations for a given term based on counting co-occurrence frequencies,
* Cluster: clustering of similar documents into the same groups,
* Summarize: summarization of important concepts in a text. Typically these are high-frequency terms,
* Categorize: classication of texts into predfned categories, and
* API: availability of application programming interfaces to extend the program with plug-ins.

### Commercial products:

* Clearforest, a text-driven business intelligence solution, 
* Copernic Summarizer, a summarizing software extracting key concepts and relevant sentences, 
* dtSearch, a document
search tool, 
* Insightful Infact, a search and analysis text mining tool, 
* Inxight, an integrated suite of tools for search, extraction, and analysis of text, *
SPSS Clementine, a data and text mining workbench, 
* SAS Text Miner, a suite of tools for knowledge discovery and knowledge
extraction in texts, 
* TEMIS, a tool set for text extraction, text clustering, and text catego-rization, 
* WordStat, a product for computer assisted text analysis.

### Open-source

* Weka suite, a collection of machine learning algorithms for data
mining tasks also offering classification and clustering techniques with extension projects for text mining, like:
* KEA for keyword extraction. 
* GATE (Cunningham et al. 2002), Text Mining Infrastructure in R, 
an established text mining framework with architecture for language processing, information extraction, ontology management and machine learning algorithms. (written in Java)
* RapidMiner (formerly Yale (Mierswa et al. 2006)), a system for knowledge discovery and data mining, 
* Pimiento (Adeva and Calvo 2006), a basic Java framework for text mining. 

However, many existing open-source products tend to offer rather
specialized solutions in the text mining context, such as:

* Shogun (Sonnenburg et al. 2006), a toolbox for string kernels, or 
* the Bow toolkit (McCallum 1996), a C library useful for statistical text analysis, language modeling and information retrieval. 

### In R

* package ttda (Mueller 2006) provides some methods for textual data analysis.


The `tm` package
----------------
text mining framework for R centered around the new package `tm` (Feinerer 2007b) = the basic infrastructure necessary to organize, transform, and analyze textual data.

R = the most versatile statistical computing environments available. However, the scope of these methods was often limited to "classical", structured input data formats (such as data frames in R). The `tm`  package =framework to apply existing methods to text data structures as well.

In addition, advanced text mining methods can be made available via extension packages, or via interfaces to established open source toolkits  So tm provides a framework for flexible integration of premier statistical methods from R, interfaces to well known open source text mining infrastructure and methods, and has a sophisticated modularized extension mechanism for text mining purposes.



Conceptual process and framework
============================================================

text mining analysis
--------------------

text mining analysis = several challenging process steps

A text mining analyst typically starts with a set of highly heterogeneous input texts. 

* first step : to **import** these texts into R. Simultaneously it is important to **organize and structure** the texts 
* second step : **tidying up**  the texts, including **preprocessing** the texts to obtain a convenient representation for later analysis. This step might involve **text reformatting** (e.g., *whitespace removal*), *stopword removal*, or *stemming procedures*. 
* Third, the analyst must be able to **transform** the preprocessed texts into structured formats to be actually computed with. For "classical" text mining tasks, this normally implies the creation of a so-called **term-document matrix**, probably the most common format to represent texts for computation. 
Now the analyst can work and compute on texts with standard techniques from statistics and data mining, like clustering or classication methods

Corpora
-------
A text mining framework must offer functionality for managing text documents, should abstract the process of document manipulation and ease the usage of heterogeneous text formats. 
Thus there is a need for a _conceptual entity similar to a database_ holding and managing text documents in a generic way: we call this entity a **text document collection** or **corpus**.

Since text documents are present in different file formats and in different locations, like a compressed file on the Internet or a locally stored text file with additional annotations, there has to be an encapsulating mechanism providing _standardized interfaces to access the document data_. We subsume this functionality in so-called **sources** .

Metadata
--------
Besides the actual textual data many modern file formats provide features to _annotate_ text documents (e.g., XML with special tags), i.e. there is metadata available which further describes and enriches the textual content and might offer valuable insights into the document structure or additional concepts.

Also, additional metadata is likely to be created during an analysis. Therefore the framework must be able to alleviate metadata usage in a convenient way, both on a document level (e.g., short summaries or descriptions of selected documents) and on a collection level (e.g., collection-wide classication tags).

Transformations
---------------
Alongside the data infrastructure for text documents the framework must provide tools and algorithms to efficiently work with the documents. That means the framework has to have functionality to perform common tasks, like whitespace removal, stemming or stopword deletion. We denote such functions operating on text document collections as **transformations**.

Another important concept is **filtering** which basically involves applying predicate functions on collections to extract patterns of interest. A surprisingly challenging operation is the one of joining text document collections. Merging sets of documents is straightforward, but merging metadata intelligently needs a more sophisticated handling, since storing metadata from different sources in successive steps necessarily results in a hierarchical, tree-like structure. The challenge is to keep these joins and subsequent look-up operations efficient for large document
collections.

Realistic scenarios in text mining use at least several hundred text documents ranging up to several hundred thousands of documents. This means a compact storage of the documents in a document collection is relevant for appropriate RAM usage - a simple approach would hold all documents in memory once read in and bring down even fully RAM equipped systems shortly with document collections of several thousands text documents. However, simple database orientated mechanisms can already circumvent this situation, e.g., by holding only pointers or hashtables in memory instead of full documents.

Text mining typically involves doing computations on texts to gain interesting information. The most common approach is to create a so-called **term-document matrix** holding frequences of distinct terms for each document. Another approach is to compute directly on character sequences as is done by **string kernel methods**. Thus the framework must allow export mecanisms for term-document matrices and provide interfaces to access the document corpora as plain character sequences.

Basically, the framework and infrastructure supplied by `tm` aims at implementing the conceptual framework presented above.



Data structures and algorithms
============================================================

Commercial text mining products (Davi et al. 2005) are typically built in monolithic structures regarding extensibility. 

We decided to tackle this problem by implementing a framework for accessing text data structures in R. We concentrated on a middle ware consisting of several text mining classes that provide access to various texts. 
On top of this basic layer we have a virtual application layer, where methods operate without explicitly knowing the details of internal text data structures. The text mining classes are written as abstract and generic as possible, so it is easy to add new methods on the application layer level. The framework uses the S4 (Chambers 1998) class system to capture an object oriented design. This design seems best capable of encapsulating several classes with internal data structures and offers typed methods to the application layer.
This modular structure enables tm to integrate existing functionality from other text mining tool kits. E.g., we interface with the `Weka` and `OpenNLP` 
 tool kits, via `RWeka` (Hornik et al.2007)|and Snowball (Hornik 2007b) for its stemmers - and `openNLP`  (Feinerer 2007a), respectively. In detail `Weka`  gives us stemming and tokenization methods, whereas `openNLP` offers amongst others tokenization, sentence detection, and part of speech tagging (Bill 1995).
We can plug in this functionality at various points in tm's infrastructure, e.g., for preprocessing via transformation methods (see Section 4), for generating term-document matrices (see Paragraph 3.1.4), or for custom functions when extending tm's methods (see Section 3.3)

`r xtag$center('![fig1](assets\\fig1.JPG)')`
`r xtag$center('**fig.1. Conceptual layers and packages**')`

Figure 1 shows both the conceptual layers of our text mining infrastructure and typical packages arranged in them. The system environment is made up of the R core and the `XML` package (Temple Lang 2006) for handling XML documents internally, the text mining framework consists of our new tm package with some help of Rstem (Temple Lang 2004) or Snowball for stemming, whereas some packages provide both infrastructure and applications, like `wordnet` (Feinerer 2007c), `kernlab` with its string kernels, or the RWeka and openNLP interfaces. A typical application might be `lsa` which can use our middleware: the key data structure for latent semantic analysis (LSA Landauer et al. 1998; Deerwester et al. 1990) is a term-document matrix which can be easily exported from our `tm` framework. As default `lsa` provides its own (rather simple) routines for generating term-document matrices, so one can either use lsa natively or enhance it with tm for handling complex input formats, preprocessing, and text manipulations, e.g., as used by Feinerer and Wild (2007).  

Data structures
----------------

We start by explaining the data structures: The basic framework classes and their interactions are depicted in Figure 2 as a UML class diagram (Fowler 2003) with implementation independent UML datatypes. In this section we give an overview how the classes interoperate and work whereas an in-depth description is found in the Appendix A to be used as detailed reference.


`r xtag$center('![fig2](assets\\fig2.JPG)')`
`r xtag$center('**Figure 2: UML class diagram of the tm package**')`
.

### Text document collections

The main structure for managing documents in `tm` is a so-called text document collection (**Corpus**). It represents a collection of text documents and can be interpreted as a database for texts. Its elements are `TextDocuments`  holding the
actual text corpora and local metadata. The text document collection has 

* two slots for storing global metadata and 
* one slot for database support.

We can distinguish two types of metadata, namely *Document Metadata* and *Collection Meta-data*. 

* Document metadata ( _DMetaData_ ) is for information specific to text documents but with an own entity, like classification results (it holds both the classifications for each documents but in addition global information like the number of classification levels). 
* Collection metadata ( _CMetaData_ ) is for global metadata on the collection level not necessarily related to single text documents, like the creation date of the collection (which is independent from the documents within the collection).

The database slot (DBControl) controls whether the collection uses a database backend for storing its information, i.e., the documents and the metadata. If activated, package `tm` tries to hold as few bits in memory as possible. The main advantage is to be able to work with very large text collections, a shortcoming might be slower access performance (since we need to load information from the disk on demand). Also note that activated database support introduces persistent object semantics since changes are written to the disk which other objects (pointers)
might be using.

Objects of class `Corpus` can be manually created by

```{r, eval=FALSE}
new("Corpus", .Data = ..., DMetaData = ..., CMetaData = ...,DBControl = ...)
```
where :
.Data has to be the list of text documents, 
and the other arguments have to be the document metadata, collection metadata and database control parameters. 
Typically, however, we use the Corpus constructor to generate the right parameters given following arguments:

object
:    a Source object which abstracts the input location.

readerControl 
:    a list with the three components reader, language, and load, giving a reader capable of reading in elements delivered from the document source, a string
giving the ISO language code (typically in ISO 639 or ISO 3166 format, e.g., en_US
for American English), and a Boolean ag indicating whether the user wants to load
documents immediately into memory or only when actually accessed (we denote this
feature as load on demand).

    The tm package ships with several readers (use getReaders() to list available readers) described in Table 2.

dbControl 
  : a list with the three components useDb, dbName and dbType setting the respective DBControl values (whether database support should be activated, the le name to
the database, and the database type).


`r xtag$center('![fig2](assets\\table2.JPG)')`
`r xtag$center('**Table 2: Available readers in the tm package**')`


An example of a constructor call might be:
```{r, eval=FALSE}
Corpus(object = ..., 
       readerControl = list(reader = object@DefaultReader,
                            language = "en_US",
                            load = FALSE),
       dbControl = list(useDb = TRUE,
                        dbName = "texts.db",
                        dbType = "DB1"))
``` 
where `object`  denotes a valid instance of class `Source.`  We will cover sources in more detaillater.


### Text documents

The next core class is a text document (TextDocument), the basic unit managed by a text document collection. It is an abstract class, i.e., we must derive specic document classes to obtain document types we actually use in daily text mining. Basic slots are 

Author 
: holding the text creators, 

DateTimeStamp 
: for the creation date, 

Description 
: for short explanations or comments,

ID
: for a unique identication string,

Origin 
: denoting the document source (like the news agency or publisher), 

Heading 
: for the document title, 

Language
: for the document language, and 

LocalMetaData 
: for any additional metadata.

The main rationale is to extend this class as needed for specific purposes. This offers great flexibility as we can handle any input format internally but provide a generic interface to other classes. 

The following four classes are derived classes implementing documents for common file formats and come with the package: *XMLTextDocument* for XML documents,
*PlainTextDocument* for simple texts, *NewsgroupDocument* for newsgroup postings and e-mails, and *StructuredTextDocument* for more structured documents (e.g., with explicitly marked paragraphs, etc.).

Text documents can be created manually, e.g., via
```{r, eval=FALSE}
new("PlainTextDocument", .Data = "Some text.", URI = uri, Cached = TRUE,
    Author = "Mr. Nobody", DateTimeStamp = Sys.time(),
    Description = "Example", ID = "ID1", Origin = "Custom",
    Heading = "Ex. 1", Language = "en_US")
```
setting all arguments for initializing the class (uri is a shortcut for a reference to the input, e.g., a call to a file on disk). In most cases text documents are returned by reader functions, so there is no need for manual construction.

### Text repositories

The next class from our framework is a so-called text repository which can be used to keep track of text document collections. The class TextRepository is conceptualized for storing representations of the same text document collection. This allows to backtrack transformations on text documents and access the original input data if desired or necessary. The dynamic slot RepoMetaData can help to save the history of a text document collection, e.g., all transformations with a time stamp in form of tag-value pair metadata.

We construct a text repository by calling
```{r, eval=FALSE}
new("TextRepository",
    .Data = list(Col1, Col2), RepoMetaData = list(created = "now"))
```
where Col1 and Col2 are text document collections.

### Term-document matrices

Finally we have a class for term-document matrices (Berry 2003; Shawe-Taylor and Cristianini 2004), probably the most common way of representing texts for further computation. It can be exported from a Corpus and is used as a bag-of-words mechanism which means that the order of tokens is irrelevant. This approach results in a matrix with document IDs as rows and terms as columns. The matrix elements are term frequencies.
For example, consider the two documents with IDs 1 and 2 and their contents text mining is fun and a text is a sequence of words, respectively. Then the term-document matrix is: 

```{r textdoc, echo=FALSE, eval=TRUE}
header <- c( "a", "fun", "is", "mining", "of", "sequence", "text", "words")
m <- matrix(data = c(1, 0, 1, 1, 1, 0, 0, 1, 0, 2, 2, 0, 1, 0, 1, 1, 1, 1), nrow = 2)
df <- as.data.frame(m)
names(df) <- header
knitr::kable(df,row.names = TRUE)
```

`TermDocMatrix`  provides such a term-document matrix for a given **Corpus** element. It has the slot **Data** of the formal class Matrix from package Matrix (Bates and Maechler 2007) to hold the frequencies in compressed sparse matrix format.


Instead of using the term frequency (weightTf) directly, one can use different weightings. The slot **Weighting** of a **TermDocMatrix** provides this facility by calling a weighting function on the matrix elements. Available weighting schemes include the binary frequency (weightBin) method which eliminates multiple entries, or the inverse document frequency (weightTfIdf) weighting giving more importance to discriminative compared to irrelevant terms. Users can apply their own weighting schemes by passing over custom weighting functions to Weighting.

Again, we can manually construct a term-document matrix, e.g., via

```{r, eval=FALSE}
new("TermDocMatrix", Data = tdm, Weighting = weightTf)
```
where tdm denotes a sparse Matrix.

Typically, we will use the TermDocMatrix constructor instead for creating a term-document matrix from a text document collection. The constructor provides a sophisticated modular structure for generating such a matrix from documents: you can plug in modules for each processing step specied via a control argument. E.g., we could use an n-gram tokenizer (NGramTokenizer) from the Weka toolkit (via RWeka) to tokenize into phrases instead of single words

```{r, eval=FALSE}
TermDocMatrix(col, control = list(tokenize = NGramTokenizer))
```

or a tokenizer from the OpenNLP toolkit (via openNLP's tokenize function)

```{r, eval=FALSE}
TermDocMatrix(col, control = list(tokenize = tokenize))
```

where col denotes a text collection. Instead of using a classical tokenizer we could be interested in phrases or whole sentences, so we take advantage of the sentence detection algorithms offered by openNLP.
```{r, eval=FALSE}
TermDocMatrix(col, control = list(tokenize = sentDetect))
```

Similarly, we can use external modules for all other processing steps (mainly via internal calls to *termFreq* which generates a term frequency vector from a text document and gives an extensive list of available control options), like stemming (e.g., the Weka stemmers via the Snowball package), stopword removal (e.g., via custom stopword lists), or user supplied dictionaries (a method to restrict the generated terms in the term-document matrix).
This modularization allows synergy gains between available established toolkits (like Weka or OpenNLP) and allows tm to utilize available functionality.

### Sources

The tm package uses the concept of a so-called source to encapsulate and abstract the document input process. This allows to work with standardized interfaces within the package without knowing the internal structures of input document formats. It is easy to add support for new file formats by inheriting from the Source base class and implementing the interface methods.
Figure 3 shows a UML diagram with implementation independent UML data types for the Source base class and existing inherited classes.


`r xtag$center('![fig3](assets\\fig3.JPG)')`
`r xtag$center('**Figure 3: UML class diagram for Sources**')`


A source is a VIRTUAL class (i.e., it cannot be instantiated, only classes may be derived from it) and abstracts the input location and serves as the base class for creating inherited classes for specialized file formats. It has four slots, namely

* LoDSupport indicating load on demand support, 
* Position holding status information for internal navigation, 
* DefaultReader for a default reader function, and 
* Encoding for the encoding to be used by internal R routines for accessing texts via the source (defaults to UTF-8 for all sources).

The following classes are specic source implementations for common purposes: 

* DirSource for directories with text documents, 
* CSVSource for documents stored in CSV files, 
* ReutersSource for special Reuters file formats, and 
* GmaneSource for so-called RSS feeds as delivered by Gmane (Ingebrigtsen 2007).

A directory source can manually be created by calling

```{r, eval=FALSE}
new("DirSource", LoDSupport = TRUE, FileList = dir(), Position = 0,
    DefaultReader = readPlain, Encoding = "latin1")
```
where readPlain() is a predefined reader function in tm. Again, we provide wrapper functions for the various sources.


Algorithms
-----------

Next, we present the algorithmic side of our framework. We start with the creation of a text document collection holding some plain texts in Latin language from Ovid's ars amato-ria (Naso 2007). Since the documents reside in a separate directory we use the DirSource
and ask for immediate loading into memory. The elements in the collection are of class PlainTextDocument since we use the default reader which reads in the documents as plain text:


```{r, eval=FALSE}
txt <- system.file("texts", "txt", package = "tm")
(ovid <- Corpus(DirSource(txt),
                readerControl = list(reader = readPlain,
                                     language = "la",
                                     load = TRUE)))
```
    A text document collection with 5 text documents


Alternatively we could activate database support such that only relevant information is kept in memory:

```{r, eval=FALSE}
Corpus(DirSource(txt),
       readerControl = list(reader = readPlain,
                            language = "la", load = TRUE),
       dbControl = list(useDb = TRUE,
                        dbName = "/home/user/oviddb",
                        dbType = "DB1"))
```


The loading and unloading of text documents and metadata of the text document collection is transparent to the user, i.e., fully automatic. Manipulations affecting R text document collections are written out to the database, i.e., we obtain persistent object semantics in contrast to R's common semantics.
We have implemented both accessor and set functions for the slots in our classes such that slot information can easily be accessed and modied, e.g.

```{r}
ID(ovid[[1]])
```
    [1] "1"

gives the ID slot attribute of the rst ovid document. With e.g.,

```{r}
Author(ovid[[1]]) <- "Publius Ovidius Naso"
```

we modify the Author slot information.

To see all available metadata for a text document, use meta(), e.g.,

```{r}
meta(ovid[[1]])
```

    Available meta data pairs are:
        Author : Publius Ovidius Naso
        Cached : TRUE
        DateTimeStamp: 2008-03-16 14:49:58
        Description :
        ID : 1
        Heading :
        Language : la
        Origin :
        URI : file /home/feinerer/lib/R/library/tm/texts/txt/ovid_1.txt
    UTF-8
    Dynamic local meta data pairs are:
    list()

Further we have implemented the following operators and functions for text document collections:

[
:   The subset operator allows to specify a range of text documents and automatically en-sures that a valid text collection is returned. Further the DMetaData data frame is automatically subsetted to the specic range of documents.
```{r}
ovid[1:3]
```
    A text document collection with 3 text documents

[[ 
:   accesses a single text document in the collection. A special `show()`  method for plain text documents pretty prints the output.
```{r}
ovid[[1]]
```
    [1] " Si quis in hoc artem populo non novit amandi,"
    [2] " hoc legat et lecto carmine doctus amet."
    [3] " arte citae veloque rates remoque moventur,"
    [4] " arte leves currus: arte regendus amor."
    [5] ""
    [6] " curribus Automedon lentisque erat aptus habenis,"
    [7] " Tiphys in Haemonia puppe magister erat:"
    [8] " me Venus artificem tenero praefecit Amori;"
    [9] " Tiphys et Automedon dicar Amoris ego."
    [10] " ille quidem ferus est et qui mihi saepe repugnet:"
    [11] ""
    [12] " sed puer est, aetas mollis et apta regi."
    [13] " Phillyrides puerum cithara perfecit Achillem,"
    [14] " atque animos placida contudit arte feros."
    [15] " qui totiens socios, totiens exterruit hostes,"
    [16] " creditur annosum pertimuisse senem."

c()
:    Concatenates several text collections to a single one.
```{r}
c(ovid[1:2], ovid[3:4])
```
    A text document collection with 4 text documents
    
The metadata of both text document collections is merged, i.e., a new root node is created in the CMetaData tree holding the concatenated collections as children, and the DMetaData data frames are merged. Column names existing in one frame but not the other are filled up with NA values. The whole process of joining the metadata is depicted in Figure 4. Note that concatenation of text document collections with
activated database backends is not supported since it might involve the generation of a new database (as a collection has to have exactly one database) and massive copying of database values.

length()
:    Returns the number of text documents in the collection.
```{r}
length(ovid)
```
```
[1] 5
```

show()
:    A custom print method. Instead of printing all text documents (consider a text collection could consist of several thousand documents, similar to a database), only a short summarizing message is printed.

summary()
:    A more detailed message, summarizing the text document collection. Available metadata is listed.
```{r}
summary(ovid)
```
```
A text document collection with 5 text documents
The metadata consists of 2 tag-value pairs and a data frame
Available tags are:
        create_date creator
Available variables in the data frame are:
        MetaID
```


inspect()
:    This function allows to actually see the structure which is hidden by show() and summary() methods. Thus all documents and metadata are printed, e.g.,
```{r}
inspect(ovid)
```

tmUpdate() 
:    takes as argument a text document collection, a source with load on demand support and a readerControl as found in the Corpus constructor. The source is checked for new files which do not already exist in the document collection. Identified new files are parsed and added to the existing document collection, i.e., the collection is updated, and loaded into memory if demanded.
```{r}
tmUpdate(ovid, DirSource(txt))
```
    A text document collection with 5 text documents
    
Text documents and metadata can be added to text document collections with `appendElem()` and `appendMeta()` , respectively. 

As already described earlier the text document collection has two types of metadata: one is the metadata on the document collection level (cmeta), the other is the metadata related to the individual documents (e.g., clusterings) (dmeta) with an own entity in form of a data frame.
```{r}
ovid <- appendMeta(ovid,
                   cmeta = list(test = c(1,2,3)),
                   dmeta = list(clust = c(1,1,2,2,2)))
summary(ovid)
```
```
A text document collection with 5 text documents

The metadata consists of 3 tag-value pairs and a data frame
Available tags are:
    create_date creator test
Available variables in the data frame are:
    MetaID clust
```
```{r}
CMetaData(ovid)
```

```
An object of class "MetaDataNode"
Slot "NodeID":
[1] 0

Slot "MetaData":
$create_date
[1] "2008-03-16 14:49:58 CET"
$creator
    LOGNAME
"feinerer"

$test
[1] 1 2 3

Slot "children":
list()
```


```{r}
DMetaData(ovid)
```
```
MetaID clust
1       0       1 
2       0       1
3       0       2
4       0       2
5       0       2
```


